{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and loading the necessary data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "from collections import defaultdict\n",
    "# libraries for the model\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "N_Cls = 10\n",
    "inDir = './data'\n",
    "DF = pd.read_csv(inDir + '/train_wkt_v4.csv')\n",
    "GS = pd.read_csv(inDir + '/grid_sizes.csv', names=['ImageId', 'Xmax', 'Ymin'], skiprows=1)\n",
    "SB = pd.read_csv(os.path.join(inDir, 'sample_submission.csv'))\n",
    "ISZ = 160\n",
    "smooth = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAbout the dataset : \\n 1km x 1km satellite images in both 3-band and 16-band formats. All images are in GeoTiff format\\n The goal is to detect and classify the types of objects found in these regions. \\n In a satellite image, you will find lots of different objects like roads, buildings, vehicles, farms, trees, water ways, etc. Dstl has labeled 10 different classes:\\n\\nBuildings - large building, residential, non-residential, fuel storage facility, fortified building\\nMisc. Manmade structures \\nRoad \\nTrack - poor/dirt/cart track, footpath/trail\\nTrees - woodland, hedgerows, groups of trees, standalone trees\\nCrops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\\nWaterway \\nStanding water\\nVehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\\nVehicle Small - small vehicle (car, van), motorbike\\n\\nEvery object class is described in the form of Polygons and MultiPolygons, which are simply a list of polygons. \\nThere are two different formats for these shapes: GeoJson and WKT. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "About the dataset : \n",
    " 1km x 1km satellite images in both 3-band and 16-band formats. All images are in GeoTiff format\n",
    " The goal is to detect and classify the types of objects found in these regions. \n",
    " In a satellite image, you will find lots of different objects like roads, buildings, vehicles, farms, trees, water ways, etc. Dstl has labeled 10 different classes:\n",
    "\n",
    "Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "Misc. Manmade structures \n",
    "Road \n",
    "Track - poor/dirt/cart track, footpath/trail\n",
    "Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "Waterway \n",
    "Standing water\n",
    "Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "Vehicle Small - small vehicle (car, van), motorbike\n",
    "\n",
    "Every object class is described in the form of Polygons and MultiPolygons, which are simply a list of polygons. \n",
    "There are two different formats for these shapes: GeoJson and WKT. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data from various formats to numpy array\n",
    "# data conversion functions\n",
    "\n",
    "def _convert_coordinates_to_raster(coords, img_size, xymax):\n",
    "    Xmax, Ymax = xymax\n",
    "    H, W = img_size\n",
    "    W1 = 1.0 * W * W / (W + 1)\n",
    "    H1 = 1.0 * H * H / (H + 1)\n",
    "    xf = W1 / Xmax\n",
    "    yf = H1 / Ymax\n",
    "    coords[:, 1] *= yf\n",
    "    coords[:, 0] *= xf\n",
    "    coords_int = np.round(coords).astype(np.int32)\n",
    "    return coords_int\n",
    "\n",
    "\n",
    "def _get_xmax_ymin(grid_sizes_panda, imageId):\n",
    "    \"\"\"\n",
    "    To resize the training polygons, we need these parameters for each image.\n",
    "    \"\"\"\n",
    "    xmax, ymin = grid_sizes_panda[grid_sizes_panda.ImageId == imageId].iloc[0, 1:].astype(float)\n",
    "    return (xmax, ymin)\n",
    "\n",
    "\n",
    "def _get_polygon_list(wkt_list_pandas, imageId, cType):\n",
    "    \"\"\"\n",
    "    Load the training polygons with shapely.\n",
    "    \"\"\"\n",
    "    df_image = wkt_list_pandas[wkt_list_pandas.ImageId == imageId]\n",
    "    multipoly_def = df_image[df_image.ClassType == cType].MultipolygonWKT\n",
    "    polygonList = None\n",
    "    if len(multipoly_def) > 0:\n",
    "        assert len(multipoly_def) == 1\n",
    "        polygonList = wkt_loads(multipoly_def.values[0])\n",
    "    return polygonList\n",
    "\n",
    "\n",
    "def _get_and_convert_contours(polygonList, raster_img_size, xymax):\n",
    "    \"\"\"\n",
    "    Create lists of exterior and interior coordinates of polygons resized to a specific image resolution.\n",
    "    \"\"\"\n",
    "    perim_list = []\n",
    "    interior_list = []\n",
    "    if polygonList is None:\n",
    "        return None\n",
    "    for k in range(len(polygonList)):\n",
    "        poly = polygonList[k]\n",
    "        perim = np.array(list(poly.exterior.coords))\n",
    "        perim_c = _convert_coordinates_to_raster(perim, raster_img_size, xymax)\n",
    "        perim_list.append(perim_c)\n",
    "        for pi in poly.interiors:\n",
    "            interior = np.array(list(pi.coords))\n",
    "            interior_c = _convert_coordinates_to_raster(interior, raster_img_size, xymax)\n",
    "            interior_list.append(interior_c)\n",
    "    return perim_list, interior_list\n",
    "\n",
    "\n",
    "def _plot_mask_from_contours(raster_img_size, contours, class_value=1):\n",
    "    \"\"\"\n",
    "    Creates a class mask (0 and 1s) from lists of exterior and interior polygon coordinates.\n",
    "    \"\"\"\n",
    "    img_mask = np.zeros(raster_img_size, np.uint8)\n",
    "    if contours is None:\n",
    "        return img_mask\n",
    "    perim_list, interior_list = contours\n",
    "    cv2.fillPoly(img_mask, perim_list, class_value)\n",
    "    cv2.fillPoly(img_mask, interior_list, 0)\n",
    "    return img_mask\n",
    "\n",
    "\n",
    "def generate_mask_for_image_and_class(raster_size, imageId, class_type, grid_sizes_panda=GS, wkt_list_pandas=DF):\n",
    "    \"\"\"\n",
    "    Outputs a specific class mask from the training images.\n",
    "    \"\"\"\n",
    "    xymax = _get_xmax_ymin(grid_sizes_panda, imageId)\n",
    "    polygon_list = _get_polygon_list(wkt_list_pandas, imageId, class_type)\n",
    "    contours = _get_and_convert_contours(polygon_list, raster_size, xymax)\n",
    "    mask = _plot_mask_from_contours(raster_size, contours, 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def M(image_id):\n",
    "    \"\"\"\n",
    "    Loads the tiff-files with different number of bands.\n",
    "    \"\"\"\n",
    "    filename = os.path.join(inDir, 'sixteen_band', '{}_M.tif'.format(image_id))\n",
    "    img = tiff.imread(filename)\n",
    "    img = np.rollaxis(img, 0, 3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def stretch_n(bands, lower_percent=5, higher_percent=95):\n",
    "    \"\"\"\n",
    "    Cuts of extreme values of spectral bands to visualize them better for the human eye.\n",
    "    \"\"\"\n",
    "    out = np.zeros_like(bands)\n",
    "    n = bands.shape[2]\n",
    "    for i in range(n):\n",
    "        a = 0  # np.min(band)\n",
    "        b = 1  # np.max(band)\n",
    "        c = np.percentile(bands[:, :, i], lower_percent)\n",
    "        d = np.percentile(bands[:, :, i], higher_percent)\n",
    "        t = a + (bands[:, :, i] - c) * (b - a) / (d - c)\n",
    "        t[t < a] = a\n",
    "        t[t > b] = b\n",
    "        out[:, :, i] = t\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def mask_for_polygons(polygons, im_size):\n",
    "    \"\"\"\n",
    "    Outputs a specific class mask from the training images.\n",
    "    \"\"\"\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "\n",
    "def mask_to_polygons(mask, epsilon=5, min_area=1.):\n",
    "    \"\"\"\n",
    "    Create a Multipolygon from a mask of 0-1 pixels.\n",
    "    \"\"\"\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    image, contours, hierarchy = cv2.findContours(\n",
    "        ((mask == 1) * 255).astype(np.uint8),\n",
    "        cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    # create approximate contours to have reasonable submission size\n",
    "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                       for cnt in contours]\n",
    "    if not contours:\n",
    "        return MultiPolygon()\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(approx_contours[idx])\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    for idx, cnt in enumerate(approx_contours):\n",
    "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
    "            assert cnt.shape[1] == 1\n",
    "            poly = Polygon(\n",
    "                shell=cnt[:, 0, :],\n",
    "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
    "                       if cv2.contourArea(c) >= min_area])\n",
    "            all_polygons.append(poly)\n",
    "    # approximating polygons might have created invalid ones, fix them\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n",
    "        # need to keep it a Multi throughout\n",
    "        if all_polygons.type == 'Polygon':\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "    return all_polygons\n",
    "\n",
    "\n",
    "def get_scalers(im_size, x_max, y_min):\n",
    "    h, w = im_size  # they are flipped so that mask_for_polygons works correctly\n",
    "    h, w = float(h), float(w)\n",
    "    w_ = 1.0 * w * (w / (w + 1))\n",
    "    h_ = 1.0 * h * (h / (h + 1))\n",
    "    return w_ / x_max, h_ / y_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preformance metrics definaton\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = torch.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = torch.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return torch.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = torch.round(torch.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = torch.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = torch.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return torch.mean(jac)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set 25\n",
      "(837, 849, 8) 6010_1_2\n",
      "(837, 849, 8) 6010_4_2\n",
      "(837, 848, 8) 6010_4_4\n",
      "(837, 848, 8) 6040_1_0\n",
      "(837, 848, 8) 6040_1_3\n",
      "(837, 848, 8) 6040_2_2\n",
      "(837, 846, 8) 6040_4_4\n",
      "(837, 851, 8) 6060_2_3\n",
      "(838, 835, 8) 6070_2_3\n",
      "(837, 848, 8) 6090_2_0\n",
      "(837, 848, 8) 6100_1_3\n",
      "(837, 848, 8) 6100_2_2\n",
      "(837, 848, 8) 6100_2_3\n",
      "(837, 849, 8) 6110_1_2\n",
      "(837, 849, 8) 6110_3_1\n",
      "(837, 849, 8) 6110_4_0\n",
      "(837, 851, 8) 6120_2_0\n",
      "(837, 851, 8) 6120_2_2\n",
      "(837, 849, 8) 6140_1_2\n",
      "(837, 849, 8) 6140_3_1\n",
      "(837, 851, 8) 6150_2_3\n",
      "(837, 848, 8) 6160_2_1\n",
      "(837, 848, 8) 6170_0_4\n",
      "(837, 848, 8) 6170_2_4\n",
      "(837, 848, 8) 6170_4_1\n"
     ]
    }
   ],
   "source": [
    "# store train set in numpy format for easy access\n",
    "def make_train_set():\n",
    "    s = 835\n",
    "\n",
    "    x = np.zeros((5 * s, 5 * s, 8))\n",
    "    y = np.zeros((5 * s, 5 * s, N_Cls))\n",
    "\n",
    "    ids = sorted(DF.ImageId.unique())\n",
    "    print(\"length of train set\",  len(ids))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            id = ids[5 * i + j]\n",
    "\n",
    "            img = M(id)\n",
    "            img = stretch_n(img)\n",
    "            print (img.shape, id)\n",
    "            x[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n",
    "            for z in range(N_Cls):\n",
    "                y[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class(\n",
    "                    (img.shape[0], img.shape[1]), id, z + 1)[:s, :s]\n",
    "\n",
    "\n",
    "\n",
    "    np.save('data/x_trn_%d' % N_Cls, x)\n",
    "    np.save('data/y_trn_%d' % N_Cls, y)\n",
    "    \n",
    "make_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape :  (4175, 4175, 8) \n",
      " mask size :  (4175, 4175, 10)\n"
     ]
    }
   ],
   "source": [
    "img = np.load('data/x_trn_%d.npy' % N_Cls)\n",
    "msk = np.load('data/y_trn_%d.npy' % N_Cls)\n",
    "print(\"image shape : \",img.shape,\"\\n mask size : \", msk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator to load data batch wise\n",
    "def get_patches(img, msk, amt=10000, aug=True):\n",
    "    is2 = int(1.0 * ISZ)\n",
    "    xm, ym = img.shape[0] - is2, img.shape[1] - is2\n",
    "\n",
    "    x, y = [], []\n",
    "\n",
    "    tr = [0.4, 0.1, 0.1, 0.15, 0.3, 0.95, 0.1, 0.05, 0.001, 0.005]\n",
    "    for i in range(amt):\n",
    "        xc = random.randint(0, xm)\n",
    "        yc = random.randint(0, ym)\n",
    "\n",
    "        im = img[xc:xc + is2, yc:yc + is2]\n",
    "        ms = msk[xc:xc + is2, yc:yc + is2]\n",
    "\n",
    "        for j in range(N_Cls):\n",
    "            sm = np.sum(ms[:, :, j])\n",
    "            if 1.0 * sm / is2 ** 2 > tr[j]:\n",
    "                if aug:\n",
    "                    if random.uniform(0, 1) > 0.5:\n",
    "                        im = im[::-1]\n",
    "                        ms = ms[::-1]\n",
    "                    if random.uniform(0, 1) > 0.5:\n",
    "                        im = im[:, ::-1]\n",
    "                        ms = ms[:, ::-1]\n",
    "\n",
    "                x.append(im)\n",
    "                y.append(ms)\n",
    "\n",
    "    x, y = 2 * np.transpose(np.array(x), (0, 3, 1, 2)) - 1, np.transpose(np.array(y), (0, 3, 1, 2))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val():\n",
    "    img = np.load('data/x_trn_%d.npy' % N_Cls)\n",
    "    msk = np.load('data/y_trn_%d.npy' % N_Cls)\n",
    "    x, y = get_patches(img, msk, amt=3000)\n",
    "\n",
    "    np.save('data/x_tmp_%d' % N_Cls, x)\n",
    "    np.save('data/y_tmp_%d' % N_Cls, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a U_NET architecture\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return self.final(dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net():\n",
    "    print (\"start train net\")\n",
    "    x_val, y_val = np.load('data/x_tmp_%d.npy' % N_Cls), np.load('data/y_tmp_%d.npy' % N_Cls)\n",
    "    img = np.load('data/x_trn_%d.npy' % N_Cls)\n",
    "    msk = np.load('data/y_trn_%d.npy' % N_Cls)\n",
    "    model = UNet11()\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 5\n",
    "\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for  data, target in get_patches(img, msk):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        data, target = x_val, y_val \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
